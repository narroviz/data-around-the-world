}
get_most_and_least_probable_pronunciation('a|b|e:r|c|r|o|m|b|i:e|')
get_most_and_least_probable_pronunciation <- function(graphemes) {
most_probable_pronunciation <- ''
most_probable_pronunciation_prob <- 1
least_probable_pronunciation <- ''
least_probable_pronunciation_prob <- 1
graphemes <- unlist(strsplit(graphemes, '[|]'))
for (i in 1:length(graphemes)) {
grapheme <- gsub('\\:', '', graphemes[i])
phonemes <- g2p_prob[[grapheme]]
most_probable_index <- which(values(phonemes) == max(values(phonemes)))
most_probable_phoneme_prob <- unname(values(phonemes)[unname(most_probable_index)])[1]
most_probable_phoneme <- strsplit(names(most_probable_index), '\\.')[[1]][1]
most_probable_pronunciation <- paste(most_probable_pronunciation, most_probable_phoneme, sep='|')
most_probable_pronunciation_prob <- most_probable_pronunciation_prob * most_probable_phoneme_prob
least_probable_index <- which(values(phonemes) == min(values(phonemes)))
least_probable_phoneme_prob <- unname(values(phonemes)[unname(least_probable_index)])[1]
least_probable_phoneme <- strsplit(names(least_probable_index), '\\.')[[1]][1]
least_probable_pronunciation <- paste(least_probable_pronunciation, least_probable_phoneme, sep='|')
least_probable_pronunciation_prob <- least_probable_pronunciation_prob * least_probable_phoneme_prob
}
return_values <- c(most_probable_pronunciation, most_probable_pronunciation_prob, least_probable_pronunciation, least_probable_pronunciation_prob)
return(return_values)
}
get_most_and_least_probable_pronunciation('a|b|e:r|c|r|o|m|b|i:e|')
paste(NULL, 'a', sep='|')
?paste
paste(NULL, 'a', collaps='|')
paste(NULL, 'a', collapse='|')
paste(NA, 'a', collapse='|')
paste(NA, 'a', sep='|')
paste(NULL, 'a', sep='|')
paste(NULL, 'a', sep='|')
paste(NULL, 'a', sep='|')[-1]
?substr
substr('abcdef', 1)
substr('abcdef', 1:)
substr('abcdef', 1:length('abcdef'))
substr('abcdef', 1, length('abcdef'))
substr('abcdef', 1, nchar('abcdef'))
get_most_and_least_probable_pronunciation <- function(graphemes) {
most_probable_pronunciation <- ''
most_probable_pronunciation_prob <- 1
least_probable_pronunciation <- ''
least_probable_pronunciation_prob <- 1
graphemes <- unlist(strsplit(graphemes, '[|]'))
for (i in 1:length(graphemes)) {
grapheme <- gsub('\\:', '', graphemes[i])
phonemes <- g2p_prob[[grapheme]]
most_probable_index <- which(values(phonemes) == max(values(phonemes)))
most_probable_phoneme_prob <- unname(values(phonemes)[unname(most_probable_index)])[1]
most_probable_phoneme <- strsplit(names(most_probable_index), '\\.')[[1]][1]
most_probable_pronunciation <- paste(most_probable_pronunciation, most_probable_phoneme, sep='|')
most_probable_pronunciation_prob <- most_probable_pronunciation_prob * most_probable_phoneme_prob
least_probable_index <- which(values(phonemes) == min(values(phonemes)))
least_probable_phoneme_prob <- unname(values(phonemes)[unname(least_probable_index)])[1]
least_probable_phoneme <- strsplit(names(least_probable_index), '\\.')[[1]][1]
least_probable_pronunciation <- paste(least_probable_pronunciation, least_probable_phoneme, sep='|')
least_probable_pronunciation_prob <- least_probable_pronunciation_prob * least_probable_phoneme_prob
}
return_values <- c(
substr(most_probable_pronunciation, 2, nchar(most_probable_pronunciation)),
most_probable_pronunciation_prob,
substr(least_probable_pronunciation, 2, nchar(least_probable_pronunciation)),
least_probable_pronunciation_prob
)
return(return_values)
}
substr('abcdef', 1, nchar('abcdef'))
get_most_and_least_probable_pronunciation('a|b|e:r|c|r|o|m|b|i:e|')
arpabet_to_ipa <- read_csv(paste(DATA_DIRECTORY, 'ARPAbet_to_IPA.csv', sep='')
)
arpabet_to_ipa <- read_csv(paste(DATA_DIRECTORY, 'ARPAbet_to_IPA.csv', sep=''))
read_csv
?read_csv
??read_csv
arpabet_to_ipa <- read.csv(paste(DATA_DIRECTORY, 'ARPAbet_to_IPA.csv', sep=''))
R_DIRECTORY <- '/Users/Daveed1/Desktop/Projects/WORDS - R/'
DATA_DIRECTORY <- paste(R_DIRECTORY, 'data/', sep='')
arpabet_to_ipa <- read.csv(paste(DATA_DIRECTORY, 'ARPAbet_to_IPA.csv', sep=''))
arpabet_to_ipa
arpabet_to_ipa
arpabet_to_ipa <- read.csv(paste(DATA_DIRECTORY, 'ARPAbet_to_IPA.csv', sep=''))
arpabet_to_ipa
get_most_and_least_probable_pronunciation('a|b|e:r|c|r|o|m|b|i:e|')
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='AH']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='B']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='ER']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='K']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='R']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='OW']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='M']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='B']
arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet=='IY']
strsplit('AB:AC', '\\:')
strsplit('AB:AC', '\\:')[[1]]
strsplit('AB', '\\:')[[1]]
strsplit('AB', '\\:')[[1]][1]
get_ipa_pronunciation_from_arpabet < function(arpabets, arpabet_to_ipa=arpabet_to_ipa) {
ipas <- ''
arpabets <- unlist(strsplit(arpabets, '[|]'))
for (i in 1:length(arpabets)) {
arpabet_split <- strsplit(arpabets[i], '\\:')[[1]]
for (j in 1:length(arpabet_split)) {
arpabet = arpabet_split[j]
ipa <- arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet==arpabet]
ipas <- paste(ipas, ipa, sep='')
}
}
return(substr(ipas, 2, nchar(ipas)))
}
get_ipa_pronunciation_from_arpabet <- function(arpabets, arpabet_to_ipa=arpabet_to_ipa) {
ipas <- ''
arpabets <- unlist(strsplit(arpabets, '[|]'))
for (i in 1:length(arpabets)) {
arpabet_split <- strsplit(arpabets[i], '\\:')[[1]]
for (j in 1:length(arpabet_split)) {
arpabet = arpabet_split[j]
ipa <- arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet==arpabet]
ipas <- paste(ipas, ipa, sep='')
}
}
return(substr(ipas, 2, nchar(ipas)))
}
get_ipa_pronunciation_from_arpabet([1] "AH|B|ER|K|R|OW|M|B|IY"                   "0.0934620797188308"
[3] "AA:N|B:UH|ER|CH:IH|AY:V|OY|M:AW|B:UH|AO" "2.00646684255952e-35"  )
get_ipa_pronunciation_from_arpabet("AH|B|ER|K|R|OW|M|B|IY")
get_ipa_pronunciation_from_arpabet(arpabets="AH|B|ER|K|R|OW|M|B|IY")
get_ipa_pronunciation_from_arpabet(arpabets="AH|B|ER|K|R|OW|M|B|IY", arpabet_to_ipa)
get_ipa_pronunciation_from_arpabet <- function(arpabets, arpabet_to_ipa) {
ipas <- ''
arpabets <- unlist(strsplit(arpabets, '[|]'))
for (i in 1:length(arpabets)) {
arpabet_split <- strsplit(arpabets[i], '\\:')[[1]]
for (j in 1:length(arpabet_split)) {
arpabet = arpabet_split[j]
ipa <- arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet==arpabet]
ipas <- paste(ipas, ipa, sep='')
}
}
return(ipas)
}
get_ipa_pronunciation_from_arpabet(arpabets="AH|B|ER|K|R|OW|M|B|IY", arpabet_to_ipa)
mapped_pronunciation[mapped_pronunciation['WORD'] == 'abercrombie']
get_ipa_pronunciation_from_arpabet(arpabets="AE|B|ER|K|R|AA|M|B|IY|", arpabet_to_ipa)
get_ipa_pronunciation_from_arpabet(arpabets="AA:N|B:UH|ER|CH:IH|AY:V|OY|M:AW|B:UH|AO", arpabet_to_ipa)
g2p_prob[['r']]
head(mapped_pronunciation)
mapped_pronunciation["Y:ER" %in% mapped_pronunciation$PHONEMES]
mapped_pronunciation["YER" %in% mapped_pronunciation$PHONEMES]
mapped_pronunciation["Y:ER" %in% mapped_pronunciation$PHONEMES]
which("Y:ER" %in% mapped_pronunciation$PHONEMES)
which("AH|" %in% mapped_pronunciation$PHONEMES)
which("AH" %in% mapped_pronunciation$PHONEMES)
grep("Y:ER", X, value = TRUE)
grep("Y:ER", mapped_pronunciation$PHONEMES, value = TRUE)
which(grep("Y:ER", mapped_pronunciation$PHONEMES, value = TRUE)
)
which(grep("Y:ER", mapped_pronunciation$PHONEMES))
grep("Y:ER", mapped_pronunciation$PHONEMES, value = TRUE)
get_ipa_pronunciation_from_arpabet(arpabets="AA:N|B:UH|ER|CH:IH|AY:V|OY|M:AW|B:UH|AO", arpabet_to_ipa)
g2p_prob[['r']]
mapped_pronunciation[mapped_pronunciation['WORD'] == 'abercrombie']
grep("AA:N", mapped_pronunciation$PHONEMES, value = TRUE)
mapped_pronunciation[mapped_pronunciation$PHONEMES=='K|AE|T|AA|L|AA:N|']
mapped_pronunciation[mapped_pronunciation$PHONEMES=='K|AE|T|AA|L|AA:N|',]
mapped_pronunciation
mapped_pronunciation_ltd <- mapped_pronunciation
nrow(mapped_pronunciation[mapped_pronunciation$IN_GRADY==TRUE | mapped_pronunciation$IN_OED==1])
nrow(mapped_pronunciation[mapped_pronunciation$IN_GRADY==TRUE | mapped_pronunciation$IN_OED==1,])
nrow(mapped_pronunciation[mapped_pronunciation$IN_GRADY==TRUE,])
nrow(mapped_pronunciation[mapped_pronunciation$IN_OED==1,])
mapped_pronunciation_ltd <- mapped_pronunciation[mapped_pronunciation$IN_GRADY==TRUE | mapped_pronunciation$IN_OED==1,]
mapped_counts_ltd <- get_grapheme_and_phoneme_mapping_counts(mapped_pronunciation_ltd)
get_ipa_pronunciation_from_arpabet <- function(arpabets, arpabet_to_ipa) {
ipas <- ''
arpabets <- unlist(strsplit(arpabets, '[|]'))
for (i in 1:length(arpabets)) {
arpabet_split <- strsplit(arpabets[i], '\\:')[[1]]
for (j in 1:length(arpabet_split)) {
arpabet = arpabet_split[j]
ipa <- arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet==arpabet]
ipas <- paste(ipas, ipa, sep='')
}
}
return(ipas)
}
g2p_ltd <- mapped_counts_ltd[[1]]
p2g_ltd <- mapped_counts_ltd[[2]]
g2p_prob_ltd <- get_probabilistic_hash_from_count_hash(g2p_ltd)
p2g_prob_ltd <- get_probabilistic_hash_from_count_hash(p2g_ltd)
mapped_pronunciation_ltd <- add_speaking_and_spelling_probabilities(mapped_pronunciation_ltd, g2p_prob_ltd, p2g_prob_ltd)
head(mapped_pronunciation_ltd)
get_most_and_least_probable_spelling <- function(phonemes, p2g_prob) {
most_probable_spelling <- ''
most_probable_spelling_prob <- 1
least_probable_spelling <- ''
least_probable_spelling_prob <- 1
phonemes <- unlist(strsplit(phonemes, '[|]'))
for (i in 1:length(phonemes)) {
phoneme <- phonemes[i]
graphemes <- p2g_prob[[phoneme]]
most_probable_index <- which(values(graphemes) == max(values(graphemes)))
most_probable_grapheme_prob <- unname(values(graphemes)[unname(most_probable_index)])[1]
most_probable_grapheme <- strsplit(names(most_probable_index), '\\.')[[1]][1]
most_probable_spelling <- paste(most_probable_spelling, most_probable_grapheme, sep='')
most_probable_spelling_prob <- most_probable_spelling_prob * most_probable_grapheme_prob
least_probable_index <- which(values(graphemes) == min(values(graphemes)))
least_probable_grapheme_prob <- unname(values(graphemes)[unname(least_probable_index)])[1]
least_probable_grapheme <- strsplit(names(least_probable_index), '\\.')[[1]][1]
least_probable_spelling <- paste(least_probable_spelling, least_probable_grapheme, sep='')
least_probable_spelling_prob <- least_probable_spelling_prob * least_probable_grapheme_prob
}
return_values <- c(most_probable_spelling, most_probable_spelling_prob, least_probable_spelling, least_probable_spelling_prob)
return(return_values)
}
get_most_and_least_probable_pronunciation <- function(graphemes, g2p_prob) {
most_probable_pronunciation <- ''
most_probable_pronunciation_prob <- 1
least_probable_pronunciation <- ''
least_probable_pronunciation_prob <- 1
graphemes <- unlist(strsplit(graphemes, '[|]'))
for (i in 1:length(graphemes)) {
grapheme <- gsub('\\:', '', graphemes[i])
phonemes <- g2p_prob[[grapheme]]
most_probable_index <- which(values(phonemes) == max(values(phonemes)))
most_probable_phoneme_prob <- unname(values(phonemes)[unname(most_probable_index)])[1]
most_probable_phoneme <- strsplit(names(most_probable_index), '\\.')[[1]][1]
most_probable_pronunciation <- paste(most_probable_pronunciation, most_probable_phoneme, sep='|')
most_probable_pronunciation_prob <- most_probable_pronunciation_prob * most_probable_phoneme_prob
least_probable_index <- which(values(phonemes) == min(values(phonemes)))
least_probable_phoneme_prob <- unname(values(phonemes)[unname(least_probable_index)])[1]
least_probable_phoneme <- strsplit(names(least_probable_index), '\\.')[[1]][1]
least_probable_pronunciation <- paste(least_probable_pronunciation, least_probable_phoneme, sep='|')
least_probable_pronunciation_prob <- least_probable_pronunciation_prob * least_probable_phoneme_prob
}
return_values <- c(
substr(most_probable_pronunciation, 2, nchar(most_probable_pronunciation)),
most_probable_pronunciation_prob,
substr(least_probable_pronunciation, 2, nchar(least_probable_pronunciation)),
least_probable_pronunciation_prob
)
return(return_values)
}
get_ipa_pronunciation_from_arpabet <- function(arpabets, arpabet_to_ipa) {
ipas <- ''
arpabets <- unlist(strsplit(arpabets, '[|]'))
for (i in 1:length(arpabets)) {
arpabet_split <- strsplit(arpabets[i], '\\:')[[1]]
for (j in 1:length(arpabet_split)) {
arpabet = arpabet_split[j]
ipa <- arpabet_to_ipa$IPA[arpabet_to_ipa$ARPAbet==arpabet]
ipas <- paste(ipas, ipa, sep='')
}
}
return(ipas)
}
get_most_and_least_probable_pronunciation("a:a|r|d|v|a|r|k|", g2p_prob_ltd)
grep("K:EY", mapped_pronunciation$PHONEMES, value = TRUE)
which(mapped_pronunciation$PHONEMES, grep("K:EY", mapped_pronunciation$PHONEMES, value = TRUE))
grep("K:EY", mapped_pronunciation$PHONEMES, value = TRUE)
mapped_pronunciation_ltd[mapped_pronunciation_ltd$PHONEMES=='D:IY|K:EY|',]
grep("K:EY", mapped_pronunciation_ltd$PHONEMES, value = TRUE)
mapped_pronunciation_ltd[mapped_pronunciation_ltd$PHONEMES=='JH:EY|EH:F|K:EY|',]
mapped_pronunciation_ltd[mapped_pronunciation_ltd$PHONEMES=='JH:EY|EH:F|K:EY|',]
mapped_pronunciation_ltd[mapped_pronunciation_ltd$PHONEMES=='OW|K:EY|',]
get_most_and_least_probable_pronunciation("a|b|d|o|m|e|n|", g2p_prob_ltd)
grep("IH:M", mapped_pronunciation_ltd$PHONEMES, value = TRUE)
mapped_pronunciation_ltd[mapped_pronunciation_ltd$PHONEMES=='_|HH|AY|IH:M|',]
save.image("~/Desktop/Projects/WORDS - R/graphemes_and_phonemes_new.RData")
getwd()
dirname()
dirname(rstudioapi::getSourceEditorContext()$path)
LONGITUDE_INCREMENT = 0.0166651236
AWS_ACCESS_KEY <- Sys.getenv("AWS_ACCESS_KEY")
AWS_SECRET_KEY <- Sys.getenv("AWS_SECRET_KEY")
S3_BUCKET <- 'article-data'
S3_FOLDER <- 'around-the-world'
Sys.setenv(
"AWS_ACCESS_KEY_ID" = AWS_ACCESS_KEY,
"AWS_SECRET_ACCESS_KEY" = AWS_SECRET_KEY,
"AWS_DEFAULT_REGION" = "us-east-2"
)
CURRENT_FILE_DIR = dirname(rstudioapi::getSourceEditorContext()$path)
#setwd("/Users/Daveed1/Desktop/Local Repos/wanderwhim/data-around-the-world/data/shapefiles")
#countries <- readOGR("ne_10m_admin_0_countries", "ne_10m_admin_0_countries")
#maritime <- readOGR("World_EEZ_v11_20191118_LR", "eez_v11_lowres")
gClip <- function(shp, bb){
if(class(bb) == "matrix") b_poly <- as(extent(as.vector(t(bb))), "SpatialPolygons")
else b_poly <- as(extent(bb), "SpatialPolygons")
gIntersection(shp, b_poly, byid = T)
}
get_line_centroids <- function(shp) {
centroid_lons <- vector()
centroid_lats <- vector()
for(i in 1:length(shp)) {
centroid_lons[i] <- mean(shp@lines[[i]]@Lines[[1]]@coords[,1])
centroid_lats[i] <- mean(shp@lines[[i]]@Lines[[1]]@coords[,2])
}
centroids <- data.frame("lon" = centroid_lons, "lat" = centroid_lats)
return(centroids)
}
get_antipode_longitude <- function(longitude) {
antipode_longitude <- (180 - abs(longitude))
if (longitude > 0) {
antipode_longitude <- -1 * antipode_longitude
}
return(antipode_longitude)
}
get_clipped_shp <- function(shp, longitude_start, longitude_end, latitude_start=-90, latitude_end=90) {
clip_bbox <- data.frame(x=c(longitude_start, longitude_end), y=c(latitude_start, latitude_end))
clipped_shp <- gClip(shp, clip_bbox)
return(clipped_shp)
}
get_shapefile_segments <- function(shp, names, territories, countries, types, longitude) {
segment_names <- c()
segment_territories <- c()
segment_countries <- c()
segment_types <- c()
start_pts <- c()
end_pts <- c()
longitudes <- c()
for(i in shp@plotOrder) {
polygon <- shp@polygons[[i]]
subpolygons <- polygon@Polygons
id <- strsplit(polygon@ID, ' ')[[1]][1]
id_row_index <- as.numeric(id) + 1
name <- as.character(names[id_row_index])
territory <- as.character(territories[id_row_index])
country <- as.character(countries[id_row_index])
# Convert codes to proper ISO3 (Natural Earth has strange, unexplained, other codes)
if (country == "CH1") {
country <- 'CHN'
} else if (country == "IS1") {
country <- 'ISR'
} else if (country == "FR1") {
country <- 'FRA'
} else if (country == "NL1") {
country <- 'NLD'
} else if (country == "FI1") {
country <- 'FIN'
} else if (country == "DN1") {
country <- 'DNK'
} else if (country == "GB1") {
country <- 'GBR'
} else if (country == "US1") {
country <- 'USA'
} else if (country == "AU1") {
country <- 'AUS'
} else if (country == "NZ1") {
country <- 'NZL'
}
type <- as.character(types[id_row_index])
for (j in 1:length(subpolygons)) {
subpolygon <- subpolygons[[j]]
subpolygon_latitudes <- subpolygon@coords[,2]
start <- min(subpolygon_latitudes)
end <- max(subpolygon_latitudes)
segment_names <- c(segment_names, name)
segment_territories <- c(segment_territories, territory)
segment_countries <- c(segment_countries, country)
segment_types <- c(segment_types, type)
start_pts <- c(start_pts, start)
end_pts <- c(end_pts, end)
longitudes <- c(longitudes, longitude)
}
}
shapefile_segment_df <- data.frame(
name=segment_names,
territory=segment_territories,
country=segment_countries,
type=segment_types,
start_latitude=start_pts,
end_latitude=end_pts,
longitude=longitudes
)
return(shapefile_segment_df)
}
filter_segments <- function(segment_df) {
filtered_df <- segment_df
unacceptable_types <- c("Joint regime", "Overlapping claim", "Disputed", "Lease", "Indeterminate")
unacceptable_indices <- which(segment_df$type %in% unacceptable_types & segment_df$country != "ATA")
if (length(unacceptable_indices) > 0) {
filtered_df <- segment_df[-unacceptable_indices,]
}
return(filtered_df)
}
are_points_nearby <- function(p1, p2) {
# If points are within 20km (globally nearby)
return(distHaversine(p1, p2) < 20000)
}
merge_segments <- function(segment_df, longitude) {
# Create a faux first row to ease rbind
merged_df <- segment_df[1,]
countries <- unique(segment_df$country)
for(i in 1:length(countries)) {
country <- countries[i]
country_indices <- which(segment_df$country == country)
country_df <- segment_df[country_indices,]
country_df <- country_df[order(country_df$start),]
if (nrow(country_df) == 1) {
merged_df <- rbind(merged_df, country_df)
} else {
merged_country_df <- country_df[1,]
num_merged_rows <- 0
current_merge_row <- country_df[1,]
for(j in 1:nrow(country_df)) {
if (j < nrow(country_df)) {
current_end <- current_merge_row$end
next_start <- country_df[j+1,]$start
next_end <- country_df[j+1,]$end
rows_are_overlapping <- current_end > next_start
rows_are_nearby <- are_points_nearby(c(longitude, current_end), c(longitude, next_start))
if (rows_are_overlapping | rows_are_nearby) {
if (next_end > current_end) {
current_merge_row$end <- next_end
}
} else {
num_merged_rows = num_merged_rows + 1
merged_country_df[num_merged_rows,] <- current_merge_row
current_merge_row <- country_df[j+1,]
}
} else {
num_merged_rows = num_merged_rows + 1
merged_country_df[num_merged_rows,] <- current_merge_row
}
# Add in the final "current merge row"
}
merged_df <- rbind(merged_df, merged_country_df)
}
}
# Delete the faux first row
merged_df <- merged_df[-1,]
merged_df <- merged_df[order(merged_df$start),]
return(merged_df)
}
get_intersection_segments <- function(longitude, longitude_end, water, land) {
clipped_water <- get_clipped_shp(water, longitude, longitude_end)
clipped_land <- get_clipped_shp(land, longitude, longitude_end)
water_segments <- get_shapefile_segments(clipped_water, water@data$TERRITORY1, water@data$ISO_TER1, water@data$ISO_SOV1, water@data$POL_TYPE, longitude)
land_segments <- get_shapefile_segments(clipped_land, land@data$ADMIN, land@data$ISO_A3, land@data$SOV_A3, land@data$TYPE, longitude)
all <- rbind(water_segments, land_segments)
filtered <- filter_segments(all)
ordered <- filtered[order(filtered$start),]
merged <- merge_segments(ordered, longitude)
return(merged)
}
get_intersections_by_index <- function(index, water, land) {
lon = -180 + (index)*LONGITUDE_INCREMENT
lon_end = -180 + (index+1)*LONGITUDE_INCREMENT
lon_intersections <- get_intersection_segments(lon, lon_end, water, land)
inv_lon = get_antipode_longitude(lon)
inv_lon_end = get_antipode_longitude(lon_end)
inv_intersections <- get_intersection_segments(inv_lon, inv_lon_end, water, land)
all_intersections <- rbind(lon_intersections, inv_intersections[order(inv_intersections$start, decreasing=TRUE),])
return(all_intersections)
}
get_elevation_distance_by_index <- function(index) {
filename = paste(index, ".json", sep="")
longitude_url <- paste("https://article-data.s3.us-east-2.amazonaws.com/", S3_FOLDER, "/", filename, sep="")
longitude_json <- fromJSON(longitude_url)
return(longitude_json)
}
combine_json_files <- function(all_intersections, longitude_json) {
countries_json <- toJSON(all_intersections)
distance_json <- toJSON(longitude_json$distance)
elevation_json <- toJSON(longitude_json$elevation)
combined_json <- paste0('{"elevation":', elevation_json, ',"distance":', distance_json, ',"countries":', countries_json, '}')
return(combined_json)
}
output_json_object_to_s3_by_index <- function(json, index) {
filename = paste(index, ".json", sep="")
output_filepath = paste(S3_FOLDER, '/', filename, sep="")
s3write_using(
x = json,
FUN = write, #jsonlite function "write"
bucket = S3_BUCKET,
object = filepath,
opts = list(acl = "public-read", multipart = FALSE, verbose = T, show_progress = T)
)
}
CURRENT_FILE_DIR
CURRENT_FILE_DIR = dirname(rstudioapi::getSourceEditorContext()$path)
source(paste(CURRENT_FILE_DIR, "/get_intersections", sep=""))
source(paste(CURRENT_FILE_DIR, "/get_intersections.R", sep=""))
source(paste(CURRENT_FILE_DIR, "/get_intersections.R", sep=""))
countries <- readOGR("/data/shapefiles/ne_10m_admin_0_countries", "ne_10m_admin_0_countries")
countries <- readOGR("/data/shapefiles/ne_10m_admin_0_countries", "/data/shapefiles/ne_10m_admin_0_countries")
maritime <- readOGR("/data/shapefiles/World_EEZ_v11_20191118_LR", "/data/shapefiles/eez_v11_lowres")
setwd(paste(CURRENT_FILE_DIR, "/data/shapefiles", sep=""))
countries <- readOGR("ne_10m_admin_0_countries", "ne_10m_admin_0_countries")
maritime <- readOGR("World_EEZ_v11_20191118_LR", "eez_v11_lowres")
i = 0
intersections <- get_intersections_by_index(i, maritime, countries)
elevation_distance <- get_elevation_distance_by_index(i)
combined_json <- combine_json_files(intersections, elevation_distance)
combined_json
output_json_object_to_s3_by_index(combined_json, i)
# Source file depndencies
CURRENT_FILE_DIR = dirname(rstudioapi::getSourceEditorContext()$path)
source(paste(CURRENT_FILE_DIR, "/get_intersections.R", sep=""))
setwd(paste(CURRENT_FILE_DIR, "/data/shapefiles", sep=""))
# Load shapefiles for maritime economic zones and country shapefiles
countries <- readOGR("ne_10m_admin_0_countries", "ne_10m_admin_0_countries")
maritime <- readOGR("World_EEZ_v11_20191118_LR", "eez_v11_lowres")
# Add and output maritime/country intersection information into s3 JSON
i = 0
intersections <- get_intersections_by_index(i, maritime, countries)
elevation_distance <- get_elevation_distance_by_index(i)
combined_json <- combine_json_files(intersections, elevation_distance)
output_json_object_to_s3_by_index(combined_json, i)
